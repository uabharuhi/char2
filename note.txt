1.目標：
讓機器利用RNN自動產生莎士比亞文本的序列
2.資料　
一個txt　模型的input 是固定長度的序列
以及一個first hidden state value

資料的問題,如果單純就讓機器看到某個seqence of X然後輸出sequence of Y
而不管first hidden state,機器能夠學習到的pattern最多就是當輸入那個X序列就會output那個Y

然而在今天我們需要的是記住一段內容甚至是整篇文章,只有靠X(假設長度為n)除非N非常大不然沒有辦法記住前文(或後文 目前先用一個方向的的RNN就好)

所以必須要靠first hidden state來達成目的 藉由first hidden state來記住前文
而first hidden state會和資料是依照怎樣的順序丟到model有關 所以不能亂shuffle


在training的時候一個batch裡面每一筆資料是以max_len的方式去分

而在generating的時候 應該是要以一個char一個char的方式
這個可以利用[None,None]的placeholder做到

而一個一個和max_len的區別因為我們有把final state接到RNN當作下一個
input 所以在FORWARD PASS的時候沒有什麼差別= = 
唯一的差別是在訓練的時候BPTT傳播的長度會和
MAX_LEN有關係 而MAX_LEN的存在就是怕SEQUENCE太長記不起來

3.debug
用tensorflow視覺化 
(gpu) D:\ml_exp\char>tensorboard --logdir=D:\ml_exp\char\log
還有這一行要加入
writer = tf.summary.FileWriter("log/", graph = sess.graph)
注意這裡路徑在windows上面用相對路徑可能用不出來
然後不用加字串的引號

4.當字元只有7000個左右的時候還是沒有辦法在training set 上面
把loss降到3以下 所以用learning rate annealing


